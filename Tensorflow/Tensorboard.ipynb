{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tensorboard to visualize tensorflow graph, plot quantitative metrics about the execution of your graph, and show addinal data like images that pass through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard operates by reading Tensorflow event files, which contain summary data that you genertae when running Tensorflow.<br>\n",
    "First create the Tensorflow graph and decide which nodes you would like to annotate<br>\n",
    "To annotate a node use 'tf.summary.scalar'. <br>\n",
    "To visualize the distributions of activations coming from a particular layer use 'tf.summary.histogram'.<br>\n",
    "Now to generate all your summary nodes use 'tf.summary.maerge_all', this steps is used so as to run all the nodes.<br>\n",
    "Now just run the 'tf.summary.FileWriter' to write this summary operation to disk<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries (var):\n",
    "    with tf.name_scope ('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('sttdev'):\n",
    "            sttdev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "        tf.summary.scalar('sttdev', sttdev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    # Adding a name scope ensures logical grouping of layers in the graph\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prb = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.droput(hidden1, keep_prob)\n",
    "\n",
    "y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identify)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(target=y_, logits=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To launch tensorboard use\n",
    "tensorboard --logdir=path/to/log-directory\n",
    "# logdir points to the directory in which FileWriter is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow program would contain millions of nodes. It would be difficult to visualize them in the starting. So we use scoping, in which variable names are scoped and the visualiation uses this information to define a hiearachy on the nodes in the graph.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('hidden') as scope:\n",
    "    a = tf.constant(5, name='alpha')\n",
    "    W = tf.Variable(tf.random_uniform([1,2], -1.0, 1.0), name = 'weights')\n",
    "    b = tf.Variable(tf.zeros([1]), name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remomber that better the name scopes better is the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow graphs have two types of connections namely data dependencies and control dependenceis.<br>\n",
    "Data Dependencies show the flow of tensors between two ops and are shown as solid arrows.<br>\n",
    "Control Dependencies use dotted lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
